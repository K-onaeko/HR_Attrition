{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8218876,"sourceType":"datasetVersion","datasetId":4871908}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kehindeonaeko/hr-attrition-logreg-and-rf-classification-model?scriptVersionId=173860329\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# HR Attrition Classification \n\nThe objective of this project is to analyze HR attrition data obtained from [Real World Fake Data](https://data.world/markbradbourne/rwfd-real-world-fake-data-season-2/workspace/file?filename=HR_Attrition.csv), which documents instances of employee attrition within a company. The aim is to assess the effectiveness of two classification methods, Logistic Regression and Random Forest, in predicting attrition based on available features.\n\nVarious classification metrics will be used to evaluate the performance of each model on the dataset. To begin the analysis, we will first import the necessary packages and load the dataset\n","metadata":{}},{"cell_type":"code","source":"# Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,\\\nf1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:44.931931Z","iopub.execute_input":"2024-04-25T04:58:44.932368Z","iopub.status.idle":"2024-04-25T04:58:44.940654Z","shell.execute_reply.started":"2024-04-25T04:58:44.932335Z","shell.execute_reply":"2024-04-25T04:58:44.939511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load dataset\ndf=pd.read_csv(\"/kaggle/input/hr-attrition/HR_Attrition.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:44.963803Z","iopub.execute_input":"2024-04-25T04:58:44.964233Z","iopub.status.idle":"2024-04-25T04:58:45.00327Z","shell.execute_reply.started":"2024-04-25T04:58:44.964199Z","shell.execute_reply":"2024-04-25T04:58:45.002075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 1470 instances and 37 features recorded in the dataset.","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:31.152709Z","iopub.execute_input":"2024-04-25T04:58:31.154403Z","iopub.status.idle":"2024-04-25T04:58:31.163418Z","shell.execute_reply.started":"2024-04-25T04:58:31.154354Z","shell.execute_reply":"2024-04-25T04:58:31.160575Z"}}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.00568Z","iopub.execute_input":"2024-04-25T04:58:45.006111Z","iopub.status.idle":"2024-04-25T04:58:45.014375Z","shell.execute_reply.started":"2024-04-25T04:58:45.006078Z","shell.execute_reply":"2024-04-25T04:58:45.012839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning and Exploration \n\nWe removed several columns from the dataset as they do not contribute useful information to the model. Specifically:\n\n1. 'Attrition Date' was dropped due to a significant number of missing values.\n2. 'StandardHours' had a constant value of 80 across all instances, providing no variability.\n3. 'Over18' was excluded as all instances had a value of \"Yes\", implying all employees are over 18.\n4. 'Random Number', 'EmployeeCount', and 'EmployeeNumber' were removed because they contain no relevant information for the model.\n\nThe dataset was then checked for missing values and duplicates. None were found, confirming the data's cleanliness and usability for analysis.","metadata":{}},{"cell_type":"code","source":"# Drop Unneeded colunms \ndf=df.drop(['Attrition Date','StandardHours','Over18','EmployeeNumber','Random Number','EmployeeCount'], axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.015651Z","iopub.execute_input":"2024-04-25T04:58:45.016003Z","iopub.status.idle":"2024-04-25T04:58:45.046096Z","shell.execute_reply.started":"2024-04-25T04:58:45.015956Z","shell.execute_reply":"2024-04-25T04:58:45.04483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check for missing_values  \nmissing_values = df.isna().sum()\nprint(missing_values)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.048727Z","iopub.execute_input":"2024-04-25T04:58:45.04913Z","iopub.status.idle":"2024-04-25T04:58:45.0594Z","shell.execute_reply.started":"2024-04-25T04:58:45.049098Z","shell.execute_reply":"2024-04-25T04:58:45.058226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for duplicate rows\nduplicate_rows = df[df.duplicated()]\nprint(\"Duplicate rows:\")\nprint(duplicate_rows)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.060818Z","iopub.execute_input":"2024-04-25T04:58:45.061156Z","iopub.status.idle":"2024-04-25T04:58:45.082128Z","shell.execute_reply.started":"2024-04-25T04:58:45.061127Z","shell.execute_reply":"2024-04-25T04:58:45.081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nIn the data exploration phase, I conducted the following steps:\n\n1. Generated a descriptive statistics table to summarize the dataset.\n2. Reviewed the different data types present in the data frame.\n3. Used a for loop to display the unique values of categorical variables, ensuring there are no errors or spelling inconsistencies.\n4. Created a graph to visualize the distribution of the attrition class weights.\n\nThe following observations were made:\n\n1. The average age of employees is 36.\n2. There are 9 columns with the data type 'object'.\n3. No inconsistencies were found in the object columns.\n4. The attrition class exhibits class imbalance, with 1233 instances labeled as (0) \"no\" and 237 instances labeled as (1) \"yes\".\n","metadata":{}},{"cell_type":"code","source":"# Gather descriptive statistics about the data\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.08339Z","iopub.execute_input":"2024-04-25T04:58:45.0837Z","iopub.status.idle":"2024-04-25T04:58:45.164154Z","shell.execute_reply.started":"2024-04-25T04:58:45.083674Z","shell.execute_reply":"2024-04-25T04:58:45.162778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check data types\nprint(df.dtypes)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.165874Z","iopub.execute_input":"2024-04-25T04:58:45.166256Z","iopub.status.idle":"2024-04-25T04:58:45.173492Z","shell.execute_reply.started":"2024-04-25T04:58:45.166226Z","shell.execute_reply":"2024-04-25T04:58:45.172197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Display unique values in your object columns\n\n# Select columns with object data types (categorical attributes)\nobject_columns = df.select_dtypes(include=['object'])\n\n# Iterate over each object column and display unique categorical contents\nfor column in object_columns:\n    print(\"Column:\", column)\n    print(df[column].value_counts())\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.174958Z","iopub.execute_input":"2024-04-25T04:58:45.175348Z","iopub.status.idle":"2024-04-25T04:58:45.199421Z","shell.execute_reply.started":"2024-04-25T04:58:45.175305Z","shell.execute_reply":"2024-04-25T04:58:45.198245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get numbers of people who left (1) vs. stayed(0)\nprint(df['Attrition'].value_counts())\nprint()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.200878Z","iopub.execute_input":"2024-04-25T04:58:45.201311Z","iopub.status.idle":"2024-04-25T04:58:45.211046Z","shell.execute_reply.started":"2024-04-25T04:58:45.201281Z","shell.execute_reply":"2024-04-25T04:58:45.209623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a stacked bart plot to visualize number of employees across department\npd.crosstab(df['Department'], df['Attrition']).plot(kind ='bar',color='mr')\nplt.title('Counts of employees who left versus stayed across department')\nplt.ylabel('Employee count')\nplt.xlabel('Department')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.216128Z","iopub.execute_input":"2024-04-25T04:58:45.216547Z","iopub.status.idle":"2024-04-25T04:58:45.735281Z","shell.execute_reply.started":"2024-04-25T04:58:45.216513Z","shell.execute_reply":"2024-04-25T04:58:45.734065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 1470 instances and 37 features recorded in the dataset.## Data Preprocessing\n\nTo handle categorical variables, we adopted two different encoding techniques:\n\n1. **Label Encoding:** We applied label encoding specifically to the 'education' column to assign the order of education levels.\n\n2. **One-Hot Encoding:** For the remaining categorical variables, we utilized one-hot encoding to preprocess them, creating binary columns for each category to represent their presence or absence.\n","metadata":{}},{"cell_type":"code","source":"# Copy the dataframe\ndf_enc = df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.736841Z","iopub.execute_input":"2024-04-25T04:58:45.737219Z","iopub.status.idle":"2024-04-25T04:58:45.742597Z","shell.execute_reply.started":"2024-04-25T04:58:45.737187Z","shell.execute_reply":"2024-04-25T04:58:45.741278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encode the `Education` column as an ordinal numeric category\ndf_enc['Education'] = (\ndf_enc['Education'].astype('category').cat.set_categories(['High School', 'Associates Degree', \"Bachelor's Degree\",\"Master's Degree\",\"Doctoral Degree\"]).cat.codes)\ndf_enc.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.744529Z","iopub.execute_input":"2024-04-25T04:58:45.745019Z","iopub.status.idle":"2024-04-25T04:58:45.777357Z","shell.execute_reply.started":"2024-04-25T04:58:45.744946Z","shell.execute_reply":"2024-04-25T04:58:45.776006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Separate features (X) and target variable (y)\nX = df_enc.drop('Attrition', axis=1)\ny = df_enc['Attrition']","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.778856Z","iopub.execute_input":"2024-04-25T04:58:45.779311Z","iopub.status.idle":"2024-04-25T04:58:45.786261Z","shell.execute_reply.started":"2024-04-25T04:58:45.779272Z","shell.execute_reply":"2024-04-25T04:58:45.784899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To maintain the same ratio of attrition class cases in our test data, we utilized the `stratify` parameter in our train-test split function, setting it equal to the target variable 'y'. This ensures that the proportions of the attrition classes remain consistent between the training and testing datasets.\n","metadata":{}},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.788219Z","iopub.execute_input":"2024-04-25T04:58:45.788692Z","iopub.status.idle":"2024-04-25T04:58:45.802255Z","shell.execute_reply.started":"2024-04-25T04:58:45.788642Z","shell.execute_reply":"2024-04-25T04:58:45.801207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We utilized a pipeline to sequentially apply a list of transformers to preprocess the data. Specifically, we used:\n\n1. **One-Hot Encoder:** This transformer was used to transform the remaining categorical variables.\n2. **Standard Scaler:** We applied this transformer to normalize numerical variables, ensuring that the weights are evenly distributed among coefficients during modeling.","metadata":{}},{"cell_type":"code","source":"# Define preprocessing steps for numerical and categorigal variables\nnumeric_features = X.select_dtypes(include=['int']).columns\ncategorical_features = X.select_dtypes(include=['object']).columns\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())  # You can include standardization if needed\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer,categorical_features)\n    ])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.804269Z","iopub.execute_input":"2024-04-25T04:58:45.804689Z","iopub.status.idle":"2024-04-25T04:58:45.815012Z","shell.execute_reply.started":"2024-04-25T04:58:45.804651Z","shell.execute_reply":"2024-04-25T04:58:45.813858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic regression model\n\nTo define the Logistic Regression Model, a pipeline is created with two main steps:\n   - **Preprocessor:** This step applies the preprocessing steps defined earlier (one-hot encoding and standard scaling).\n   - **Classifier:** This step specifies the logistic regression classifier.\n","metadata":{}},{"cell_type":"code","source":"# Define the logistic regression model\nlogreg_model = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression())\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.816968Z","iopub.execute_input":"2024-04-25T04:58:45.817576Z","iopub.status.idle":"2024-04-25T04:58:45.824184Z","shell.execute_reply.started":"2024-04-25T04:58:45.817544Z","shell.execute_reply":"2024-04-25T04:58:45.823228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the logistic regression model\nlogreg_model = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression())\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.826305Z","iopub.execute_input":"2024-04-25T04:58:45.826701Z","iopub.status.idle":"2024-04-25T04:58:45.837396Z","shell.execute_reply.started":"2024-04-25T04:58:45.826669Z","shell.execute_reply":"2024-04-25T04:58:45.836031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model provides us with an accuracy of 86%. That is what portion of the predictions the model produced was correct. For further analysis, we will build a confusion matrix and use other metrics such as precision, recall and F1 score to evaluate the model\n","metadata":{}},{"cell_type":"code","source":"# Fit the model\nlogreg_model.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = logreg_model.predict(X_test)\n\n# Evaluate the model\naccuracy = logreg_model.score(X_test, y_test)\nprint(\"Logistic Regression Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.838945Z","iopub.execute_input":"2024-04-25T04:58:45.839312Z","iopub.status.idle":"2024-04-25T04:58:45.963587Z","shell.execute_reply.started":"2024-04-25T04:58:45.839284Z","shell.execute_reply":"2024-04-25T04:58:45.962445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Confusion matrix for log reg model\n\nThe confusion matrix provides a detailed breakdown of the classification model's performance. It provides insights into how effectively the model predicts each class, showing counts of true positives, true negatives, false positives, and false negatives. In this instance:\n\n- True Negatives (TN): 236 instances correctly predicted as negative or \"did not leave\".\n- False Positives (FP): 11 instances incorrectly predicted as positive or \"left\" when they did not.\n- False Negatives (FN): 28 instances incorrectly predicted as negative or \"did not leave\" when they left.\n- True Positives (TP): 19 instances correctly predicted as positive or \"left\".\n","metadata":{}},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, y_pred)\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:45.965099Z","iopub.execute_input":"2024-04-25T04:58:45.966243Z","iopub.status.idle":"2024-04-25T04:58:46.479418Z","shell.execute_reply.started":"2024-04-25T04:58:45.9662Z","shell.execute_reply":"2024-04-25T04:58:46.47837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This classification report provides insights into the model's performance:\n\n- Precision: \n  - For predicting \"no attrition,\" the precision is 89%, meaning 89% of the instances classified as \"no attrition\" were correctly classified.\n  - For predicting \"attrition,\" the precision is 63%, indicating that only 63% of instances classified as \"attrition\" were correct.\n\n- Recall:\n  - For \"no attrition,\" the recall is 96%, indicating that 96% of actual instances of \"no attrition\" were correctly classified.\n  - For \"attrition,\" the recall is 40%, indicating that only 40% of actual instances of \"attrition\" were correctly classified.\n\n- F1-score:\n  - The F1-score is the harmonic mean of precision and recall, providing a balance between the two. \n  - For \"no attrition,\" the F1-score is 92%, while for \"attrition,\" it is 49%.\n","metadata":{}},{"cell_type":"code","source":"# Classification report for logistic regression model\ntarget_names = ['Predicted no attrition', 'Predicted attrition']\nprint(classification_report(y_test, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:46.481081Z","iopub.execute_input":"2024-04-25T04:58:46.481555Z","iopub.status.idle":"2024-04-25T04:58:46.500071Z","shell.execute_reply.started":"2024-04-25T04:58:46.481511Z","shell.execute_reply":"2024-04-25T04:58:46.498647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_proba = logreg_model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:46.501696Z","iopub.execute_input":"2024-04-25T04:58:46.502078Z","iopub.status.idle":"2024-04-25T04:58:46.803641Z","shell.execute_reply.started":"2024-04-25T04:58:46.502043Z","shell.execute_reply":"2024-04-25T04:58:46.80251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest Classifier\n\nTo define the Random Forest Model, we follow the same pipeline steps used in the logistic regression model:\n   - **Preprocessor:** This step applies the preprocessing steps defined earlier (one-hot encoding and standard scaling).\n   - **Classifier:** This step specifies the logistic regression classifier.","metadata":{}},{"cell_type":"code","source":"# Define the Random Forest Classifier model\nrf_model = RandomForestClassifier(random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:46.805826Z","iopub.execute_input":"2024-04-25T04:58:46.806415Z","iopub.status.idle":"2024-04-25T04:58:46.812269Z","shell.execute_reply.started":"2024-04-25T04:58:46.80637Z","shell.execute_reply":"2024-04-25T04:58:46.810871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the pipeline including preprocessing steps\nrf_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', rf_model)\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:46.814172Z","iopub.execute_input":"2024-04-25T04:58:46.814582Z","iopub.status.idle":"2024-04-25T04:58:46.824409Z","shell.execute_reply.started":"2024-04-25T04:58:46.81455Z","shell.execute_reply":"2024-04-25T04:58:46.823257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model provides us with an accuracy of 85%. That is what portion of the predictions the model produced was correct. For further analysis, we will build a confusion matrix and use other metrics such as precision, recall and F1 score to evaluate the model\n","metadata":{}},{"cell_type":"code","source":"# Fit the model\nrf_pipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf_pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Random Forest Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:46.826219Z","iopub.execute_input":"2024-04-25T04:58:46.826544Z","iopub.status.idle":"2024-04-25T04:58:47.292169Z","shell.execute_reply.started":"2024-04-25T04:58:46.826518Z","shell.execute_reply":"2024-04-25T04:58:47.290774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion matrix for Random Forest Classifier model\n\nThe confusion matrix provides a detailed breakdown of the classification model's performance. It provides insights into how effectively the model predicts each class, showing counts of true positives, true negatives, false positives, and false negatives. In this instance:\n\n- True Negatives (TN): 243 instances correctly predicted as negative or \"did not leave\".\n- False Positives (FP): 4 instances incorrectly predicted as positive or \"left\" when they did not.\n- False Negatives (FN): 39 instances incorrectly predicted as negative or \"did not leave\" when they left.\n- True Positives (TP): 8 instances correctly predicted as positive or \"left\".\n","metadata":{}},{"cell_type":"code","source":"# create confusion matrix\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:47.294065Z","iopub.execute_input":"2024-04-25T04:58:47.294701Z","iopub.status.idle":"2024-04-25T04:58:47.722358Z","shell.execute_reply.started":"2024-04-25T04:58:47.29466Z","shell.execute_reply":"2024-04-25T04:58:47.721182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This classification report provides insights into the model's performance:\n\n1. **Precision**: The precision for \"attrition\" is 0.67, which is slightly higher than that of the logistic regression model (0.63). This means that out of all instances predicted as attrition by the random forest model, 67% are correctly classified.\n\n2. **Recall**: The recall for \"attrition\" is 0.17, which is lower than that of the logistic regression model (0.40). This indicates that the random forest model correctly identifies only 17% of all actual attrition cases, which is lower than the logistic regression model.\n\n3. **F1-score**: The F1-score for \"attrition\" is 0.27, which is also lower than that of the logistic regression model (0.49).\n\n4. **Accuracy**: The accuracy of the random forest model is 0.85, slightly lower than the logistic regression model's accuracy of 0.87.","metadata":{}},{"cell_type":"code","source":"# print classification report\ntarget_names = ['no attrition', 'attrition']\nprint(classification_report(y_test, y_pred, target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:47.724088Z","iopub.execute_input":"2024-04-25T04:58:47.724768Z","iopub.status.idle":"2024-04-25T04:58:47.743927Z","shell.execute_reply.started":"2024-04-25T04:58:47.724726Z","shell.execute_reply":"2024-04-25T04:58:47.742663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print ROC curve\ny_pred_proba = rf_pipeline.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T04:58:47.745774Z","iopub.execute_input":"2024-04-25T04:58:47.746165Z","iopub.status.idle":"2024-04-25T04:58:48.028342Z","shell.execute_reply.started":"2024-04-25T04:58:47.746133Z","shell.execute_reply":"2024-04-25T04:58:48.027353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing the two models, the logistic regression model has higher recall, F1-score, and accuracy for predicting attrition compared to the random forest model. However, the random forest model has a slightly higher precision for predicting attrition. \n\n**In conclusion, our main concern is correctly identifying as many attrition cases as possible (higher recall). Therefore, the logistic regression model is more suitable.**\n","metadata":{}}]}